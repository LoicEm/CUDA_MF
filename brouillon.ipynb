{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda.reduction import ReductionKernel\n",
    "\n",
    "# Calcule l'erreur de la matrice\n",
    "error_kernel = ReductionKernel(dtype_out=np.float32, neutral=\"0\",\n",
    "                     reduce_expr=\"a+b\", map_expr=\"(x[i]-y[i])*(x[i]-y[i])\",\n",
    "                     arguments= \"float *x, float *y\")\n",
    "# NE FONCTIONNE PAS POUR L'INSTANT, on utilise un ElementwistKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Petit benchmark\n",
    "\n",
    "Crois-les, ils sont longs à relancer et ça met longtemps\n",
    "\n",
    "Pour vérifier la mémoire utilisée sur la cg, nvidia-smi\n",
    "\n",
    "A priori ElementwiseKernel est bien le plus rapide, en plus la mémoire est mieux gérée qu'en tentant les opérations algébriques directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "error_kernel = ElementwiseKernel(\"const float *x, const float *y, float *z\",\n",
    "                                \"z[i] = (x[i] - y[i])*(x[i] - y[i])\",\n",
    "                                \"error_kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(10000, 10000).astype(np.float32)\n",
    "b = np.random.randn(10000, 10000).astype(np.float32)\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "diff_matrix = gpuarray.empty_like(a_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "cuMemAlloc failed: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-04e30e86ae1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'((a_gpu - b_gpu)*(a_gpu - b_gpu)).get() #\\xa0En plus soucis de mémoire'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2131\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPUArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_like_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_common_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axpbyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36m_new_like_me\u001b[0;34m(self, dtype, order)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         return self.__class__(self.shape, dtype,\n\u001b[0;32m--> 387\u001b[0;31m                 allocator=self.allocator, strides=strides, order=order)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;31m# operators ---------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cudaenv/lib/python3.6/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, dtype, allocator, base, gpudata, strides, order)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgpudata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: cuMemAlloc failed: out of memory"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "((a_gpu - b_gpu)*(a_gpu - b_gpu)).get() # En plus soucis de mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.3 ms ± 482 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "error_kernel(a_gpu , b_gpu, diff_matrix) # Quand le code est-il exécuté ? Seulement quand on get le truc ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373 ms ± 368 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "k = (a-b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.6 ms ± 544 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "k = diff_matrix.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la pénalité pour chaque matrice pour éviter l'overfitting\n",
    "a = np.arange(8).reshape(4,2)\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b = np.arange(1, 10).reshape(3,3)\n",
    "b_gpu = gpuarray.to_gpu(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Met tous les élements de la matrice au carré\n",
    "square_kernel = ElementwiseKernel(\"const float *x\", \"float *y\",\n",
    "                                  \"y[i] = x[i]*x[i]\",\n",
    "                                  \"square_kernel\")\n",
    "\n",
    "# Somme ligne par ligne\n",
    "line_sum_mod = SourceModule(\"\"\"\n",
    "                        #include <stdio.h>\n",
    "                        \n",
    "                        __global__ void sumcolumns (const long *a, long *b)\n",
    "                        {\n",
    "                            int ncols = 2;\n",
    "                            int idx_start = threadIdx.x;\n",
    "                            long sum = 0;\n",
    "                            for (int idx = ncols*idx_start; idx < ncols*idx_start + ncols; idx++)\n",
    "                                sum += a[idx];\n",
    "                            b[idx_start] = sum;\n",
    "                        }\"\"\")\n",
    "\n",
    "\n",
    "# Somme colonne par colonne\n",
    "column_sum_mod = SourceModule(\"\"\"\n",
    "                        #include <stdio.h>\n",
    "                        \n",
    "                        __global__ void sumcolumns (const long *a, long *b)\n",
    "                        {\n",
    "                            int nlines = 4;\n",
    "                            int ncols = 2; \n",
    "                            int idx_start = threadIdx.x + threadIdx.y*ncols;\n",
    "                            long sum = 0;\n",
    "                            for (int idx = 0; idx <  nlines; idx++)\n",
    "                                sum += a[idx_start + ncols*idx];\n",
    "                            b[idx_start] = sum;\n",
    "                        }\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "lsummod = line_sum_mod.get_function(\"sumcolumns\")\n",
    "csummod = column_sum_mod.get_function(\"sumcolumns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sum_columns = gpuarray.zeros((1,2), dtype=int)\n",
    "csummod(a_gpu, out_sum_columns, block=(2,2,1)) # Pourquoi les sommes pour (4,1,1) et (2,2,1) ne sont pas équivalentes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 16]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sum_columns.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sum_lines = gpuarray.zeros((1,4), dtype=int)\n",
    "lsummod(a_gpu, out_sum_lines, block=(32,32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  9, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sum_lines.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(real_matrix, predicted_matrix, l_u, l_v, p, q):\n",
    "    # Faire la matrice des différences au carré,\n",
    "    # Calcule le coût de p\n",
    "    # Calcule celui de q\n",
    "    return (real_matrix - predicted_matrix)**2 + l_u*norm(p) + l_v * norm(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication des matrices p et q\n",
    "\n",
    "Nécessaire pour obtenir l'erreur totale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda.tools import DeviceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplication ligne par ligne (ou colonne par colonne)\n",
    "# On peut cacher les données de la ligne\n",
    "# Que faire quand il y en a trop ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la matrice principale\n",
    "\n",
    "On va se concentrer sur un modèle qui fonctionne, donc on crée une matrice où l'on sait qu'il existe une solution.\n",
    "Pour cela, on crée une des matrices originales P_o et Q_o que l'on multiplie.\n",
    "\n",
    "On note que contrairement à une véritable dataset, la matrice sera ici très dense avec chaque cellule contenant une valeur. Dans la vraie vie (Notes sur Netflix, Amazon, etc...), les matrices comprennent énormément de NaNs, qu'on ne retrouvera pas ici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer deux \"vraies\" matrices P et Q, faire le produit\n",
    "p_o = np.random.randn(100, 1000) # Generate from uniform(0, 1)\n",
    "q_o = np.random.randn(1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On en profite pour faire un benchmark de la vitesse de mutliplication de matrice par numpy et par cuda\n",
    "On remarque d'ailleurs que la différence de facteur augmente au fur et à mesure qu'on augmente la taille de la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05 ms ± 320 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit np.matmul(q_o, p_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_matmul = SourceModule(\"\"\"\n",
    "    #include <stdio.h>\n",
    "    \n",
    "    __global__ void prodbyline (const float *p, const float *q, float *r, const int nq, const int np, const int ncom)\n",
    "    {\n",
    "    printf(\"__NEW__\");\n",
    "    // TODO : cache the line\n",
    "    const int startp = (threadIdx.x + threadIdx.y + threadIdx.z)* ncom;\n",
    "    const int startq = threadIdx.x + threadIdx.y + threadIdx.z; \n",
    "    const int startr = (threadIdx.x + threadIdx.y + threadIdx.z) * nq;\n",
    "    printf(\"P : %d, Q : %d, R : %d\\\\n\", startp, startq, startr);\n",
    "    \n",
    "    for (int idcell = startr; idcell < startr + nq; idcell++)\n",
    "     {\n",
    "        float sumcell = 0;\n",
    "        for(int idy = startq, idx = startp;\n",
    "            idy <= startq + nq*ncom;\n",
    "            idy += nq, idx++)\n",
    "            sumcell += p[idx] * q[idy];\n",
    "            printf(\"For id %d, value is %d\\\\n\", idcell, sumcell);\n",
    "            r[idcell] = sumcell;\n",
    "            }\n",
    "        }\n",
    "            \"\"\")\n",
    "#Todo : corriger ça"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1, 11).reshape(2,5).astype(np.float32)\n",
    "b = a.T.copy()\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul = line_matmul.get_function(\"prodbyline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9223372034707292159, 9223372034707292159],\n",
       "       [9223372034707292159, 9223372034707292159]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_mat = gpuarray.zeros((2,2), dtype=int)\n",
    "matmul(a_gpu, b_gpu, out_mat, np.int32(5), np.int32(5), np.int32(2), block=(4,1,1))\n",
    "out_mat.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que faire quand + de lignes que de threads possible ?\n",
    "- Modulo nb total de threads\n",
    "- \"maillage\" en python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation du grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridmod = SourceModule(\"\"\"\n",
    "    #include <stdio.h>\n",
    "    \n",
    "    __global__ void reperage (float *a)\n",
    "    {\n",
    "    printf(\"__NEW__\");\n",
    "    const int idx = threadIdx.x + 5*threadIdx.y;\n",
    "    a[0] = idx;\n",
    "    printf(\"Block is (%d, %d), thread is (%d, %d):\\\\n\", blockIdx.x, blockIdx.y, threadIdx.x, threadIdx.y);\n",
    "    }\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryprint = gridmod.get_function('reperage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryprint(a_gpu, block=(2,1,1), grid=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   2.,   3.,   4.,   5.],\n",
       "       [  6.,   7.,   8.,   9.,  10.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_gpu.get() # ??? Seulement executé quand on appelle get ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension d'un block\n",
    "Je ne comprends pas exactement quel est l'intérêt d'écrire un block en plus d'une dimension. Est-ce que la vitesse est meilleure dans certains cas pour (32, 16, 2) que (1024,1,1) ?\n",
    "A tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
